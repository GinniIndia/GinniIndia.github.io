<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="UTF-8" />
      <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <title>Ginni Garg - Portfolio</title>
      <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600&display=swap" rel="stylesheet" />
      <style>
         :root {
         --primary: #5c6bc0;
         --secondary: #3949ab;
         --light: #f0f4ff;
         --dark: #1a237e;
         --card-bg: rgba(255, 255, 255, 0.9);
         }
         body {
         font-family: 'Poppins', sans-serif;
         background: linear-gradient(120deg, #e3f2fd, #ede7f6);
         margin: 0;
         padding: 0;
         color: #333;
         min-height: 100vh;
         display: flex;
         justify-content: center;
         align-items: flex-start;
         }
         .container {
         width: 95%;
         max-width: 1200px;
         margin: 20px auto;
         padding: 20px;
         background: var(--card-bg);
         backdrop-filter: blur(8px);
         border-radius: 16px;
         box-shadow: 0 8px 25px rgba(0,0,0,0.15);
         text-align: justify;
         }
         /* Header */
         .header {
         text-align: center;
         margin-bottom: 20px;
         }
         .header h1 {
         font-size: 2.5rem;
         color: var(--dark);
         margin: 0;
         }
         .header p {
         font-size: 1.1rem;
         color: var(--secondary);
         margin-top: 6px;
         }
         /* Tabs */
         .tabs {
         display: flex;
         overflow-x: auto;
         gap: 16px;
         border-bottom: 2px solid #ddd;
         margin-bottom: 16px;
         padding-bottom: 4px;
         scrollbar-width: none;
         }
         .tabs::-webkit-scrollbar { display: none; }
         .tab {
         flex: 0 0 auto;
         padding: 10px 14px;
         cursor: pointer;
         font-weight: 600;
         color: var(--secondary);
         position: relative;
         transition: color 0.3s ease;
         white-space: nowrap;
         }
         .tab.active {
         color: var(--dark);
         }
         .tab.active::after {
         content: '';
         position: absolute;
         bottom: -2px;
         left: 0;
         right: 0;
         height: 3px;
         background: var(--secondary);
         border-radius: 3px;
         }
         /* Content */
         .tab-content {
         display: none;
         animation: fadeIn 0.4s ease-in-out;
         }
         .tab-content.active { display: block; }
         @keyframes fadeIn {
         from {opacity: 0; transform: translateY(10px);}
         to {opacity: 1; transform: translateY(0);}
         }
         h2 {
         color: var(--dark);
         margin-top: 10px;
         margin-bottom: 15px;
         border-left: 5px solid var(--secondary);
         padding-left: 10px;
         font-size: 1.3rem;
         }
         /* Cards */
         .card {
         background: #fff;
         border-radius: 12px;
         box-shadow: 0 4px 12px rgba(0,0,0,0.08);
         padding: 16px;
         margin-bottom: 16px;
         transition: transform 0.3s ease, box-shadow 0.3s ease;
         }
         .card:hover {
         transform: translateY(-3px);
         box-shadow: 0 6px 18px rgba(0,0,0,0.15);
         }
         .card h3 { margin-top: 0; color: var(--secondary); }
         .about-card {
         display: flex;
         justify-content: space-between;
         align-items: flex-start;
         gap: 20px;
         flex-wrap: wrap;
         }
         .about-content { flex: 1; min-width: 250px; }
         .about-image { flex-shrink: 0; text-align: center; }
         .about-image img {
         width: 160px;
         height: 160px;
         object-fit: cover;
         border-radius: 50%;
         border: 4px solid var(--secondary);
         box-shadow: 0 4px 15px rgba(0,0,0,0.15);
         transition: transform 0.3s ease;
         }
         .about-image img:hover { transform: scale(1.05); }
         ul { padding-left: 20px; margin: 0; }
         ul li { margin-bottom: 8px; }
         table {
         width: 100%;
         border-collapse: collapse;
         margin: 10px 0;
         font-size: 0.95rem;
         }
         th, td {
         border: 1px solid #cfd8dc;
         padding: 8px;
         text-align: left;
         }
         /* Courses */
         .course-row {
         display: flex;
         justify-content: space-between;
         align-items: center;
         margin-bottom: 8px;
         flex-wrap: wrap;
         }
         .align-right a {
         text-decoration: none;
         color: var(--secondary);
         font-weight: 500;
         }
         .align-right a:hover { text-decoration: underline; }
         a { color: var(--secondary); text-decoration: none; }
         a:hover { text-decoration: underline; }
         /* Floating Chatbot */
         .chatbot-widget {
         position: fixed;
         bottom: 20px;
         right: 20px;
         width: 400px;
         height: 500px;
         border: 2px solid #5c6bc0;
         border-radius: 16px;
         box-shadow: 0 4px 20px rgba(0,0,0,0.3);
         background: white;
         display: flex;
         flex-direction: column;
         z-index: 10000;
         overflow: hidden;
         display: none; /* hidden by default */
         }
         .chatbot-header {
         background: #5c6bc0;
         color: white;
         padding: 10px 14px;
         font-weight: 600;
         cursor: pointer;
         display: flex;
         justify-content: space-between;
         align-items: center;
         }
         .chatbot-body {
         flex: 1;
         display: flex;
         }
         .chatbot-iframe {
         width: 100%;
         height: 100%;
         border: none;
         }
         /* Small round toggle button */
         .chatbot-toggle {
         position: fixed;
         bottom: 20px;
         right: 20px;
         background: #5c6bc0;
         color: white;
         font-size: 22px;
         width: 55px;
         height: 55px;
         border-radius: 50%;
         box-shadow: 0 4px 12px rgba(0,0,0,0.3);
         display: flex;
         justify-content: center;
         align-items: center;
         cursor: pointer;
         z-index: 10001;
         transition: transform 0.2s ease;
         }
         .chatbot-toggle:hover {
         transform: scale(1.1);
         }
         
         /* Responsive iframe container */
	.iframe-wrapper {
	  position: relative;
	  width: 100%;
	  max-width: 100%;
	  height: 80vh;           /* takes 80% of viewport height */
	  min-height: 500px;     /* safe minimum for desktop */
	  border-radius: 16px;
	  overflow: hidden;
	  box-shadow: 0 6px 18px rgba(0,0,0,0.15);
	 }

	/* iframe fills container */
	.iframe-wrapper iframe {
	  width: 100%;
	  height: 100%;
	  border: none;
	 }
	 
	 
	 @media (max-width: 768px) {
	  .iframe-wrapper {
	    height: 70vh;
	    min-height: 420px;
	  }
	 }

	@media (max-width: 480px) {
	  .iframe-wrapper {
	    height: 65vh;
	    min-height: 380px;
	  }
	}

         
         /* Responsive */
         @media (max-width: 768px) {
         .header h1 { font-size: 2rem; }
         .chatbot-widget {
         width: 90%;
         height: 70%;
         right: 5%;
         }
         .header p { font-size: 1rem; }
         .about-card { flex-direction: column-reverse; align-items: center; text-align: center; }
         .about-content { text-align: justify; }
         table, th, td { font-size: 0.85rem; }
         .course-row { flex-direction: column; align-items: flex-start; gap: 4px; }
         }
         @media (max-width: 480px) {
         .container { padding: 12px; }
         .chatbot-widget {
         width: 90%;
         height: 70%;
         right: 5%;
         }
         h2 { font-size: 1.1rem; }
         .card { padding: 12px; }
         .tab { padding: 8px 10px; font-size: 0.9rem; }
         .about-image img { width: 130px; height: 130px; }
         }
      </style>
   </head>
   <body>
      <div class="container">
         <!-- Header -->
         <div class="header">
            <h1>GINNI GARG</h1>
            <p>Software Engineer | Generative AI and NLP | Research Author | Problem Solver</p>
         </div>
         <!-- Tabs -->
         <div class="tabs">
            <div class="tab active" onclick="openTab('about')">About</div>
            <div class="tab" onclick="openTab('projects')">Projects</div>
            <div class="tab" onclick="openTab('Certifications')">Certifications</div>
            <div class="tab" onclick="openTab('skills')">Skills</div>
            <div class="tab" onclick="openTab('research')">Research</div>
            <div class="tab" onclick="openTab('achievements')">Achievements</div>
            <div class="tab" onclick="openTab('profile')">Profile</div>
            <div class="tab" onclick="openTab('contact')">Contact</div>
            <div class="tab" onclick="openTab('evolution')">Evolution</div>
            <div class="tab" onclick="openTab('face-ai-demo')">Face AI Demo</div>
         </div>
         <!-- About -->
         <div id="about" class="tab-content active">
            <div class="card about-card">
               <div class="about-content">
                  <h2>About Me</h2>
                  <p>I am Ginni Garg, a passionate software professional with 4+ years of experience in the IT industry.
                     I graduated from NIT Kurukshetra, securing a position among the Top 5 students of my department.
                     Over the years, I’ve had the opportunity to work with reputed organizations including Arcesium, Otipy, SirionLabs, and C-DOT, where I contributed to building scalable backend systems and solving complex engineering challenges.
                     With strong proficiency in software backend development, my current interests are focused on Generative AI, NLP, and cutting-edge applications of Large Language Models (LLMs). I enjoy bridging the gap between traditional backend engineering and modern AI-driven systems to deliver impactful solutions. 
                  </p>
               </div>
               <div class="about-image">
                  <img src="./portfolio-pic/upsc_photo.jpg" alt="Ginni Garg">
               </div>
            </div>
            <div class="card">
               <h2>Education</h2>
               <table>
                  <thead>
                     <tr>
                        <th>Grade</th>
                        <th>Institute</th>
                        <th>Duration</th>
                        <th>CGPA/%</th>
                     </tr>
                  </thead>
                  <tbody>
                     <tr>
                        <td>B.Tech CSE</td>
                        <td>NIT Kurukshetra</td>
                        <td>2016-2020</td>
                        <td>9.65</td>
                     </tr>
                     <tr>
                        <td>12th</td>
                        <td>D.A.V. Public School</td>
                        <td>2014-2015</td>
                        <td>91</td>
                     </tr>
                     <tr>
                        <td>10th</td>
                        <td>D.A.V. Public School</td>
                        <td>2012-2013</td>
                        <td>10</td>
                     </tr>
                  </tbody>
               </table>
            </div>
            <div class="card">
               <h2>Experience</h2>
               <h3>Scientist ‘B’ (Senior Software Engineer) @ CDOT, Delhi (April 2024 - Present)</h3>
               <ul>
                  <li><b>Three Factor OAuth Service:</b> Developed a scalable Microservice from scratch based on three factor of authentication JWT, TOTP and Digital Signature. Involves Client registration, Users registration, Locking User, Ngnix Gateway, HTTPs for SSL/TLS. Further app is deployed over K8, with pvc logging, autoscaling / load balancing using hpa. PVC storage never be full with logging as deletion of logs will follow automatically using FIFO after reaching defined threshold 80-90% of PVC storage space. <br>
                     <b>Tech Used:</b> Flask, Docker, MySQL, K8, PVC, Ngnix, HTTPs, Ingress, Encryption using RSA.
                  </li>
                  <li><b>Open Source Key Management System (KMS):</b> Lead end-to-end KMS solutiong using Keycloak as User Mangement and Hashicorp Vault as Secret Key Manager. System involves three microservices, one is wrapper microservice to have single point of access to both keycloak and hashicorp-vault microservice resp. Two Factor Authentication using JWT and TOTP. Data can be stored from files or any other text in HashiCorp Vault as secret. PVC storage never be full with logging as deletion of logs will follow automatically using FIFO after reaching defined threshold 80-90% of PVC storage space. Two type of users “Admin and Normal Users”, where admin users can do read/write both while normal users can do only read operations. Used Hashicorp Vault for secret storage as its more secure becoz of encryption compare to secrets of K8 which are simply base64 encoded. <br>
                     <b>Tech Used:</b> Flask, Docker, MySQL, K8, PVC, Ngnix, HTTPs, Ingress, Hashicorp Vault, Keycloak.
                  </li>
                  <li><b>Call Data Records (CDR):</b> Project involves extracting all call histories for mobile number for a given time period. CDR works as middle software between client and TSP. All data is kept as encrypted. Self Designed SSLRestTemplate for HTTPs for client end. Enabled Authentication using AES 256 bits encryption and Hashing validation. Setup entire Testing Setup of CDR. <br>
                     <b>Tech Used:</b> Java Spring, MySQL, K8, PVC, Ngnix, HTTPs, Ingress, Encryption using AES.
                  </li>
               </ul>
               <h3>Software Engineer – II @ SirionLabs, Gurugram (Dec 2022 – May 2023)</h3>
               <ul>
                  <li><b>AutoExtractionPython:</b> Developed a highly Async Microservice Application to manage legal contracts of clients, which involves parsing data from pdf documents using OCR, then doing predictions for type of legal contract, NER for various MetaFields, Extraction of Title from Documents, Documents Cosine Similarity. <br>
                     <b>Tech Used:</b> Flask, Spacy, NLP, Roberta, Regex, Pulsar, Python, Docanno, Model Training/Validation.
                  </li>
               </ul>
               <h3>Software Development Engineer – 2 @ Otipy, Gurugram (May 2022 – Nov 2022)</h3>
               <ul>
                  <li><b>Inventory Management System:</b> Initiated an inventory service to manage cart checkouts with a high throughput over 2,000 transactions per second, utilizing Redis-lock as a distributed locking mechanism.</li>
                  <li><b>Supplier Incentive:</b> Conceptualized and implemented an incentive mechanism based on their product quality, resulting in greater supplier involvement and enhanced product quality.</li>
                  <li><b>Optimization:</b> Optimized the existing codes for faster data retrieval from databases thereby enhancing the key consumer APIs.</li>
                  <li><b>Warehouse Management System:</b> ➢Automated warehouse management system to generate quick and decisive data for all stakeholders.<br>
                     <b>Tech Used:</b> Django, SQL, Redis, Celery, Kafka, Pagination, Postman, S3, Debugging, Authentication
                  </li>
               </ul>
               <h3>Software Engineer @ Arcesium (Remote) (Aug 2020 – May 2022)</h3>
               <ul>
                  <li><b>Carbon:</b> Worked on various Deshaw Finance Reports such as CAT, Fx and Wire Order. We build CAT Report from scratch which includes calculation of Back Office (BO) numbers which are given to Front Office (De-Shaw). Also includes various feature such Dividend Reconciliation, Total Swap Reset, Mandatory/Voluntary and Treasury Data. <br> <b>Tech used:</b> ETL Framework, Flask Framework, Sqlite3 in-memory, YAML, Gunicorn Server, Unit Test Cases, Authentication, Threading.</li>
                  <li><b>Python Scripts:</b> ➢Worked on various independent python scripts for various Clients – Morgan Stanley, Coinbase, Black Stone etc., which involves processing trading/crypto trading data as per their requirements and giving output in form of CSV and Excel. <br>
                     <b>Tech Used:</b> ETL Framework, Flask Framework, Async Await Python, Python Scripting, Sqlite3 in-memory db,
                     Postgres SQL, YAML, Gunicorn Server, Unit Test Cases (For Sync and Async Python), Git, Gitlab, S3 Buckets,
                     Authentication – Kerberos and JWT, JIRA, Debugging, Threading and Multi-processing.
                  </li>
               </ul>
            </div>
         </div>
         <!-- Projects -->
         <div id="projects" class="tab-content">
            <div class="card">
               <h2>Highlighted Projects</h2>
               <h3>Semantic Search Engine and Question-Answering over Github DataSets/issues</h3>
               <p>Developed a Semantic Search Engine to get best n=5 matching issues for user search from Github huggingface DataSets repo using FAISS index, further In-context learning is used to have question answering system over n matching issues for particular user search using T-5 model. App is deployed over Hugging Face Spaces. Further preprocessed data of github hugging_face dataset repo is pushed to huggingface hub.<br>
                  <b>Tech:</b> Gradio, DataSets, Cosine Similarity, Cls_Pool (model: sentence-transformers/multi-qa-mpnet-base-dot-v1), Hugging Face Hub, T-5 for QA,  FAISS.
               </p>
               <h3>Designed a Fast Tokenizer from Scratch over WikiText Dataset</h3>
               <p>Designed a Fast Tokenizer over WikiText dataset of batch size 1000 as Generator, further various tokenization steps such as Normalization (NFD, Lowercase, StripAccents),  Pre-tokenization (Whitespace and Punctuation), Model and Trainer (BPE, WordPiece, Unigram), Post Processing ([CLS], [SEP]), Wrapped Tokenzier using PreTrainedTokenizerFast. End to End build Wrapped Tokenizer is further pushed to hub over huggingface models.<br>
                  <b>Tech:</b> Transformers, WordPiece, WikiText Dataset, Python, HuggingFace hub. 
               </p>
               <h3>English to French Translator by Fine-Tuning MarianMT and KDE4 Dataset</h3>
               <p>Developed a English to French Translator by fine-tuning existing model MarianMT by using KDE4 en-to-fr dataset. Fine Tuning using Trainer API, invovles 3 epochs, sacrebleu evaluation metric, split into train and validation in 9:1 with seed 20. Sacrebleu score improved from 38 to 52 approx 14% improvement due to fine-tuning. Further app deployed over huggingface spaces.<br>
                  <b>Tech:</b> Gradio, Transformers, KDE4 dataset, Sacrebleu Metric, Model : Helsinki-NLP/opus-mt-en-fr, data_collator (Dynamic Padding)
               </p>
               <h3>Detoxify Dialogue Summarization by Fine Tuning using Reinforcement Learning (PPO and Reward Model)</h3>
               <p>Developed end-to-end LLM application to Detoxify Dialogue Summarization using RLHF, which involves using LoRA PEFT Flan-T5 as base model, further Value Head for PPO added additional parameters 768 + 1 (Bias). Facebook Roberta hate speech detection model is used as Reward Model. KL Divergence is used between ref_model and ppo_model to have additional reward. Comparative study of detoxification between ref_model to ppo_model is done by measuring reward score respectively.<br>
                  <b>Tech:</b> Dataset : knkarthick/dialogsum, LoRA PEFT Flan-T5, Facebook Roberta Hate Speech as Reward Model, KL Divergence, PPO, Amazon SageMaker AI
               </p>
               <h3>AI powered Semantic Search engine using RAG for Fashion Store</h3>
               <p>Developed end to end AI powered Semantic Search engine using RAG for Fashion Store. RAG system involves FAQ’s,  and Product information (Technical or Creative accordingly choose Temperature and top_p model params), Cls_pool (model: sentence-transformers/multi-qa-mpnet-base-dot-v1) as Semantic Embedding in Weaviate Db (Hybrid Search using weight, Semantic Search and Keyword Search), Flan-T5 for QA after semantic search , Phoenix and Open-Telemetry for observation and evaluation of project.<br>
                  <b>Tech:</b> Gradio, Cls_pool (model: sentence-transformers/multi-qa-mpnet-base-dot-v1) as Semantic Embedding in Weaviate Db,  Flan-T5, Phoenix and Open-Telemetry
               </p>
               <h3>Designed Naive Bayes Classifier from Scratch</h3>
               <p>Architected a Naive Bayes classifier from scratch for spam email classification using Bayesian probability theory. Applied feature independence assumptions while implementing conditional probability distributions for word occurrences. Integrated Laplace smoothing to ensure all vocabulary elements appear in both spam and non-spam training sets, eliminating P(word|class)=0 scenarios. Enhanced numerical stability through logarithmic transformation of probability scores, preventing misclassification due to floating-point underflow. Delivered a robust classification system achieving 98% accuracy on testing datasets<br>
                  <b>Tech:</b> Python, Bayesian Probability, Increasing Function (Log)
               </p>
               <h3> FakeFinder – AI-Generated Image Detection System (CNN Trained from Scratch)</h3>
               <p>
                  Developed <strong>FakeFinder</strong>, an end-to-end deep learning system to distinguish
                  <strong>AI-generated images</strong> from <strong>real images</strong> using flexible
                  convolutional neural networks (CNNs) and automated hyperparameter optimization.
               <ul>
                  <li>
                     Designed <strong>dynamically configurable CNN architectures</strong> using
                     <code>nn.Sequential</code>, enabling flexible adjustment of network depth, filter sizes,
                     kernel dimensions, and regularization parameters without hard-coding model structures.
                  </li>
                  <li>
                     Constructed a <strong>robust hyperparameter search space</strong> and implemented a
                     custom <strong>Optuna objective function</strong> to systematically optimize model
                     performance.
                  </li>
                  <li>
                     Executed <strong>Optuna studies</strong> to automatically explore and identify effective
                     CNN configurations across multiple trials.
                  </li>
                  <li>
                     Evaluated models using both <strong>predictive performance</strong> and
                     <strong>efficiency metrics</strong> (accuracy, model size, inference time) to support
                     deployment-aware decision making.
                  </li>
                  <li>
                     Applied advanced <strong>model selection strategies</strong>, including:
                     <ul>
                        <li>Weighted scoring to balance accuracy and computational efficiency</li>
                        <li>Constraint-based filtering to enforce strict limits on resource usage</li>
                     </ul>
                  </li>
                  <li>
                     Refined and selected optimal CNN models by analyzing trade-offs between accuracy and
                     performance constraints.
                  </li>
               </ul>
               <b>Tech:</b> PyTorch, Flexible CNN Architectures, Optuna (Tree-Structured Parzen Estimator (TPE) is a Bayesian optimization algorithm), Model Selection (Weighted &amp; Constraint-Based),
               AI-Generated vs Real Images Dataset (subset)
               <table>
                  <caption>Optuna Hyperparameter Optimization Results</caption>
                  <thead>
                     <tr>
                        <th>number</th>
                        <th>acc</th>
                        <th>batch</th>
                        <th>dropout_rate</th>
                        <th>fc_size</th>
                        <th>k_s_l0</th>
                        <th>k_s_l1</th>
                        <th>k_s_l2</th>
                        <th>lr</th>
                        <th>n_f_l0</th>
                        <th>n_f_l1</th>
                        <th>n_f_l2</th>
                        <th>n_layers</th>
                        <th>resolution</th>
                     </tr>
                  </thead>
                  <tbody>
                     <tr>
                        <td>15</td>
                        <td>0.732</td>
                        <td>16</td>
                        <td>0.170192</td>
                        <td>384</td>
                        <td>5</td>
                        <td>3</td>
                        <td>NaN</td>
                        <td>0.000732</td>
                        <td>32</td>
                        <td>24</td>
                        <td>NaN</td>
                        <td>2</td>
                        <td>16</td>
                     </tr>
                     <tr>
                        <td>9</td>
                        <td>0.715</td>
                        <td>8</td>
                        <td>0.200294</td>
                        <td>384</td>
                        <td>5</td>
                        <td>3</td>
                        <td>NaN</td>
                        <td>0.001072</td>
                        <td>24</td>
                        <td>16</td>
                        <td>NaN</td>
                        <td>2</td>
                        <td>16</td>
                     </tr>
                     <tr>
                        <td>6</td>
                        <td>0.715</td>
                        <td>16</td>
                        <td>0.134501</td>
                        <td>384</td>
                        <td>3</td>
                        <td>3</td>
                        <td>3</td>
                        <td>0.000135</td>
                        <td>64</td>
                        <td>56</td>
                        <td>16</td>
                        <td>3</td>
                        <td>16</td>
                     </tr>
                     <tr>
                        <td>16</td>
                        <td>0.692</td>
                        <td>16</td>
                        <td>0.151364</td>
                        <td>256</td>
                        <td>5</td>
                        <td>3</td>
                        <td>NaN</td>
                        <td>0.000109</td>
                        <td>32</td>
                        <td>40</td>
                        <td>NaN</td>
                        <td>2</td>
                        <td>16</td>
                     </tr>
                     <tr>
                        <td>1</td>
                        <td>0.690</td>
                        <td>16</td>
                        <td>0.395933</td>
                        <td>128</td>
                        <td>3</td>
                        <td>5</td>
                        <td>NaN</td>
                        <td>0.001220</td>
                        <td>56</td>
                        <td>16</td>
                        <td>NaN</td>
                        <td>2</td>
                        <td>32</td>
                     </tr>
                  </tbody>
               </table>
               </p>
               <h3> FakeFinder – AI-Generated Image Detection System (Fine-Tune MobileNetV3-Large)</h3>
               <p>
                  Developed a deep learning–based image classification system to distinguish AI-generated images from real images using a subset of the AI-Generated Images vs Real Images dataset. Implemented an end-to-end training and evaluation pipeline using PyTorch, including dataset loading, image preprocessing, data augmentation, and efficient batching through DataLoaders.
                  Fine-tuned a pre-trained MobileNetV3-Large model by freezing its feature extraction layers and replacing the final classifier head to adapt the network for binary classification. Applied transfer learning techniques to significantly improve model performance with minimal training.
                  Achieved 82% test accuracy with just one training epoch using MobileNetV3-Large, outperforming a CNN model trained from scratch, which achieved ~72% accuracy after three epochs, demonstrating the effectiveness of transfer learning for limited-data scenarios.<br>
                  <b>Tech:</b> PyTorch, MobileNetV3-Large, Transfer Learning, Image Classification, Data Augmentation, CNNs
               </p>
               <h3> Pneumonia Diagnostic AI Assistant — Chest X-Ray Classification</h3>
               <p>
                  Built an image classification model to distinguish Normal, Bacterial Pneumonia, and Viral Pneumonia using the Chest X-Ray dataset. Implemented a clean training pipeline with PyTorch Lightning, including a LightningDataModule for preprocessing and a LightningModule encapsulating ResNet18 (fine-tuned on the classifier head and last two conv layers), loss, Accuracy metrics, and Adam optimizer. Configured a Lightning Trainer with EarlyStopping (callback), ModelCheckpoint, and LR scheduling (ReduceLROnPlateau) to automate training. Achieved ~90% test accuracy with limited epochs.<br>
                  <b>Tech:</b> PyTorch Lightning, ResNet18 (partial fine-tuning), AdamW, ReduceLROnPlateau, EarlyStopping
               </p>
               <h3> ArcFace-Weaviate : Face Recognition-Based Authentication System </h3>
               <p> Developed an application enabling registration of new users using facial images and subsequent identification/authentication based on stored faces. Implemented embedding generation using ArcFace, producing 512-dimensional vectors for each registered face. Used Weaviate DB with HNSW (Hierarchical Navigable Small World) graphs to efficiently index and search embeddings using cosine similarity. Authentication logic: if cosine similarity > 60%, the input face is considered a match. Built a user-friendly Gradio interface for image input, registration, and real-time identification. Applied transfer learning for feature extraction, ensuring high accuracy with minimal data. Optimized workflow for fast search and retrieval of embeddings, supporting scalable face-based authentication.<br>
               <b>Tech:</b> Python, ArcFace, Gradio, Weaviate, Transfer Learning
               </p>
               <h3> AI Powered Visual Search Engine for Fashion Store </h3>
               <p>Developed a deep learning–based visual search system that retrieves visually similar fashion items. Trained a MobileNetV2 classifier on the clothing-dataset-small (7 apparel classes) using Cross-Entropy Loss, then leveraged the trained backbone in a Siamese Network for metric learning with TripletMarginLoss. Images were resized to 64×64, producing 276-dimensional embeddings optimized with AdamW. The system performs cosine similarity search to return the top-5 nearest items, enabling accurate and efficient visual product discovery. The approach fully utilizes class labels for initial supervised training before embedding-based similarity learning.<br>
               <b>Tech:</b> Python, MobileNetV2, Siamese Network, CrossEntropyLoss, TripletMargin Loss, AdamW, Cosine Similarity, DataSet : clothing-dataset-small
               </p>
               <h3>CleanVision AI – Edge Model Optimization </h3>
               <p>
               Optimized a ResNet-18 image classification model for CPU-only edge devices used in smart city street-cleaning vehicles. Applied pruning, INT8 dynamic quantization, and quantization-aware training (QAT) to meet strict latency and storage constraints. Reduced model size from ~512 MB to <150 MB and CPU inference latency from ~900 ms to <50 ms per image. Maintained >95% classification accuracy across clean, litter, and recycle categories. Enabled real-time, GPU-free deployment on low-resource embedded hardware.
               <b>Tech:</b> PyTorch, ResNet, Model Pruning, INT8 Quantization, QAT, Edge AI
               </p>
            </div>
         </div>
         <!-- Courses -->
         <div id="Certifications" class="tab-content">
            <div class="card">
               <h2>Coursera and DeepLearning.AI</h2>
               <ul>
                  <li class="course-row">
                     Retrieval Augmented Generation (RAG)
                     <span class="align-right">
                     <a href="https://www.coursera.org/account/accomplishments/verify/A9OWYXWZR81V" target="_blank">
                     Certificate
                     </a>
                     </span>
                  </li>
                  <li class="course-row">Generative AI with Large Language Models  <span class="align-right"> <a href="https://www.coursera.org/account/accomplishments/records/UVHGQ4D3U4A4" target="_blank">Certificate</a></span></li>
                  <li class="course-row">ChatGPT Prompt Engineering for Developers  <span class="align-right"> <a href="https://learn.deeplearning.ai/accomplishments/d0aacb45-d38e-4ba8-b721-5b845152ef44?usp=sharing" target="_blank">Certificate</a></span></li>
                  <li class="course-row">LangChain for LLM Application Development  <span class="align-right"> <a href="https://learn.deeplearning.ai/accomplishments/9be46612-853b-47aa-9679-4516f974da52?usp=sharing" target="_blank">Certificate</a></span></li>
                  <li class="course-row">Machine Learning in Production  <span class="align-right"> <a href="https://www.coursera.org/account/accomplishments/verify/9N15X32F5PA8" target="_blank">Certificate</a></span></li>
                  <li class="course-row">Trustworthy AI: Managing Bias, Ethics, and Accountability  <span class="align-right"> <a href="https://www.coursera.org/account/accomplishments/verify/6N9G1OH8Z19D" target="_blank">Certificate</a></span></li>
                  <li class="course-row">Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization  <span class="align-right"> <a href="https://www.coursera.org/account/accomplishments/verify/GYY1WR5ZBHZB" target="_blank">Certificate</a></span></li>
                  <li class="course-row">Artificial Intelligence: Ethics & Societal Challenges <span class="align-right"> <a href="https://www.coursera.org/account/accomplishments/verify/WYCK9P0VMQ80" target="_blank">Certificate</a></span></li>
                  <li class="course-row">Securing AI and Advanced Topics <span class="align-right"> <a href="https://www.coursera.org/account/accomplishments/verify/E3XIO490EPBY" target="_blank">Certificate</a></span></li>
                  <li class="course-row">AI Infrastructure and Operations Fundamentals <span class="align-right"> <a href="https://www.coursera.org/account/accomplishments/verify/IMH413QSW2A8" target="_blank">Certificate</a></span></li>
                  <li class="course-row">Human-Centered Artificial Intelligence <span class="align-right"> <a href="https://www.coursera.org/account/accomplishments/verify/OD5ATAPO8VV3" target="_blank">Certificate</a></span></li>
                  <li class="course-row">Unsupervised Learning, Recommenders, Reinforcement Learning <span class="align-right"> <a href="https://www.coursera.org/account/accomplishments/verify/9M6BRLI4BY5D" target="_blank">Certificate</a></span></li>
                  <li class="course-row">Mathematics for Machine Learning and Data Science Specialization <span class="align-right"> <a href="https://www.coursera.org/account/accomplishments/specialization/HKY54NYNTF79" target="_blank">Certificate</a></span></li>
                  <li class="course-row">Linear Algebra for Machine Learning and Data Science <span class="align-right"> <a href="https://www.coursera.org/account/accomplishments/verify/93C2DW8F2LO1" target="_blank">Certificate</a></span></li>
                  <li class="course-row">Calculus for Machine Learning and Data Science <span class="align-right"> <a href="https://www.coursera.org/account/accomplishments/verify/ROZI2BLED66B" target="_blank">Certificate</a></span></li>
                  <li class="course-row">Probability & Statistics for Machine Learning & Data Science <span class="align-right"> <a href="https://www.coursera.org/account/accomplishments/verify/EJFMJKKU6021" target="_blank">Certificate</a></span></li>
                  <li class="course-row">PyTorch: Fundamentals <span class="align-right"> <a href="https://www.coursera.org/account/accomplishments/verify/2EM5PXXZ8AOR" target="_blank">Certificate</a></span></li>
                  <li class="course-row">PyTorch: Techniques and Ecosystem Tools <span class="align-right"> <a href="https://www.coursera.org/account/accomplishments/verify/MLKFYRH19FTD" target="_blank">Certificate</a></span></li>
                  <li class="course-row">PyTorch: Advanced Architectures and Deployment <span class="align-right"> <a href="https://www.coursera.org/account/accomplishments/verify/XVJH2T70TY6U" target="_blank">Certificate</a></span></li>
                  <li class="course-row">PyTorch for Deep Learning Specialization <span class="align-right"> <a href="https://www.coursera.org/account/accomplishments/specialization/Z3DF6RU88BJO" target="_blank">Certificate</a></span></li>
                  
                  <li class="course-row">AI for Medical Diagnosis <span class="align-right"> <a href="https://www.coursera.org/account/accomplishments/verify/DH56JHBQNH1A" target="_blank">Certificate</a></span></li>
               </ul>
            </div>
            <div class="card">
               <h2>Hugging Face Courses</h2>
               <ul>
                  <li class="course-row">LLM Course  <span class="align-right"> <a href="https://drive.google.com/file/d/19r0HGdmz_7s78U3z-hGksKTTZo9_p9Sv/view?usp=sharing" target="_blank">Certificate</a> <a href="./certificates/hugging_face2.pdf" target="_blank">Certificate</a> </span></li>
                  <li class="course-row">Hugging Face Agents Course  <span class="align-right"> <a href="https://huggingface.co/datasets/agents-course/final-certificates/resolve/main/certificates/ginnigarg/2025-08-27.png" target="_blank">Certificate</a>   <a href="./certificates/agent-certi-1.jpg" target="_blank">Certificate</a>  </span></li>
                  <li class="course-row">MCP Course  <span class="align-right"> <a href="./certificates/mcp-certi-2.jpg" target="_blank">Certificate</a> <a href="./certificates/mcp-certi-1.jpg" target="_blank">Certificate</a></span></li>
               </ul>
            </div>
            <div class="card">
               <h2>Udemy Courses</h2>
               <ul>
                  <li class="course-row">NLP with Python  <span class="align-right"> <a href="https://www.udemy.com/certificate/UC-1694edea-ddca-4dd8-aba6-0cab424113f8/" target="_blank">Certificate</a></span></li>
                  <li class="course-row">Python Programming  <span class="align-right"> <a href="https://www.udemy.com/certificate/UC-ef0f8a21-aaf3-4cea-bec5-8cba86dba12b/" target="_blank">Certificate</a></span></li>
               </ul>
            </div>
            <div class="card">
               <h2>Non Technical Courses</h2>
               <ul>
                  <li class="course-row">The Science of Well-Being  <span class="align-right"> <a href="https://www.coursera.org/account/accomplishments/verify/DL3EL0BUFOR1" target="_blank">Certificate</a></span></li>
               </ul>
            </div>
         </div>
         <!-- Skills -->
         <div id="skills" class="tab-content">
            <div class="card">
               <h2>NLP and Generative AI Skills</h2>
               <ul>
                  <li>Flan-T5, Fine-Tuning, Tokenizer, Hugging Face, SpaCy, NLP, NER, FAISS, Regex, Gradio, BERT</li>
                  <li>LLM, Generative AI, RAG, Agentic AI, MCP, KL Divergence, Transformers, Weaviate (Vector DB)</li>
                  <li>Keyword Search (TF-IDF and BM25), Semantic Search, Hybrid Search (Reciprocal Ranking Fusion : RRF, Weight based Fusion)</li>
                  <li>Reinforcement Learning (PPO, GRPO), LoRA: PEFT, MLOps, Phoenix and Open Telemetry</li>
                  <li>Smolagents, Ollama, Qwen, Chain-of-Thought Reasoning, ReAct Model, Langchain, LangGraph</li>
                  <li>Linear Algebra, Calculus, Probability & Statistics for Machine Learning & Data Science</li>
                  <li>Trustworthy AI, Responsible AI, Fairness, Baises, Ethics & Societal Challenges in AI</li>
                  <li>Content based Filtering Recommendation System, PCA, Dataset, DataLoader (Pinned Memory), Gradient Accumulation, Transfer Learning, Transforms (Data Augmentation)</li>
                  <li>TorchVision, Pytorch Lightning, Profiler, Partial/Full Fine-Tuning, Resnet18, ImageNet, Step LR, Reduce LR on Plateau, and Cosine Annealing LR</li>
                  <li>Flexible CNN Architectures, Optuna (Tree-Structured Parzen Estimator (TPE) is a Bayesian optimization algorithm), Model Selection (Weighted & Constraint-Based)</li>
                  <li>Siamese Network (Embedding Model, TripletMarginLoss), Resnet (Inputs added to blocks Output), Densenet (Inputs are concat to blocks Output, channel up)</li>
                  <li>Encoder, Decoder, Encoder-Decorder NLP, Stable Diffusion, MLFlow, ONNX, Unstructered/Structured/Global Pruning, Static/Dynamic/QAT Quantization</li>
               </ul>
            </div>
            <div class="card">
               <h2>Backend Skills</h2>
               <ul>
                  <li>Backend: Flask, Spring MVC, Django, Golang (Echo)</li>
                  <li>Encryption (AES, RSA), SSL/TLS, Watermarking, Authentication (Keycloak, JWT, Digital Signature)</li>
                  <li>Databases: MySQL, MongoDB, Postgres, Weaviate (Vector DB)</li>
                  <li>DevOps: Kubernetes, Docker, Git, GitLab, CI/CD</li>
               </ul>
            </div>
         </div>
         <!-- Research -->
         <div id="research" class="tab-content">
            <div class="card">
               <h2>Research Papers</h2>
               <h3>Land Cover Classification using Satellite Images and Hybrid MLP-SVM Classifier</h3>
               <a href="https://arxiv.org/abs/2101.00214" target="_blank">Research Paper</a>
               <p>Main objective of this project is to take satellite/Hyperspectral images of any area and then perform 
                  classification into different classes like – crop classification using Hybrid MLP-SVM Classifier.<br>
                  <b>Tech:</b> Python, MLP, SVM, Hyperspectral Images, Feature Extraction
               </p>
               <h3>Brain Tumor Segmentation and Detection using Ensemble Classifier</h3>
               <a href="https://arxiv.org/abs/2101.00216" target="_blank">Research Paper</a>
               <p>Main objective of this project is to take MRI Images of brain, which are further segmented and classifier using ensemble classifier. Various technologies are used like Otsu’s method is used for segmentation, feature extraction is done using PCA+SWT+GLCM, then segmentation is done using KNN+DT+RF ensemble classifier.<br>
                  <b>Tech:</b> PCA, SWT, GLCM, KNN, Decision Tree, Random Forest, Machine Learning
               </p>
               <h3>Real Time Plant Disease Detection and Classification</h3>
               <a href="https://arxiv.org/abs/2101.00215" target="_blank">Research Paper</a>
               <p>Research project based on Real Time plant leaf disease identification and classification using various techniques like ANN, K-means segmentation.<br>
                  <b>Tech:</b> Machine Learning, Image Processing, K-means, Neural Network
               </p>
            </div>
            <div class="card">
               <h2> Research Reviewer </h2>
               <ul>
                  <li class="course-row">Springer Nature </li>
                  <li class="course-row">SAGE Publishing </li>
                  <li class="course-row"> Public Library of Science (PLOS) </li>
                  <li class="course-row"> FACTA UNIVERSITATIS : Electronics and Energetics </li>
                  <li class="course-row"> IAES : International Journal of Artificial Intelligence <span class="align-right">
                     <a href="./certificates/review_certificate_3.pdf" target="_blank">
                     Certificate
                     </a>
                     </span>
                  </li>
                  <li class="course-row"> IAES : International Journal of Informatics and Communication Technology <span class="align-right">
                     <a href="./certificates/reviewer_certi_2025_ijct.pdf" target="_blank">
                     Certificate
                     </a>
                     </span>
                  </li>
                  <li class="course-row"> The Institute of Engineers (India) : Series A <span class="align-right">
                     <a href="./certificates/review_certificate_1.pdf" target="_blank">
                     Certificate
                     </a>
                     </span>
                  </li>
               </ul>
            </div>
         </div>
         <!-- Achievements -->
         <div id="achievements" class="tab-content">
            <div class="card">
               <h2>Achievements</h2>
               <ul>
                  <li>2× Academic Excellence Awards</li>
                  <li>3× Published Research Papers</li>
                  <li>15× Research Reviewer</li>
                  <li>2x Direct PhD Selection at IIT Ropar and Jodhpur</li>
                  <li><b>Department Rank:</b> 4 | <b>University Rank:</b> 5</li>
                  <li><b>JEE Main:</b> 8123 (231/360) | <b>GATE:</b> 2562</li>
                  <li>Ranked 252 in National Science Talent Search Examination</li>
                  <li>Selected in Indo-Asian Research Week with Google. Selected among top 50 Students from all the applicants in Computer Vision Track. </li>
                  <li>Member of Institution Innovation Council under the ageis of MHRD’s Innovation Cell established at NIT, Kurukshetra for academic year 2018-2019.</li>
               </ul>
            </div>
            
            <div class="card">
            	<h2>Books (Recreational Activities)</h2>
            	<ul>
            		<li>"The Psychology of Money" by Morgan Housel</li>
            		<li>"The Power of Now" by Eckhart Tolle</li>
            		<li>"Ikigai: The Japanese Secret to a Long and Happy" by Héctor García and Francesc Miralles</li>
            		<li>"The Power of Your Subconscious Mind" by Dr. Joseph Murphy</li>
            		<li>"Atomic Habits" by James Clear</li>
            		<li>"How To Stop Worrying And Start Living" by Dale Carnegie</li>
            		<li>"How to Win Friends and Influence People" by Dale Carnegie</li>
            		<li>"The 21 Irrefutable Laws of Leadership" by John C. Maxwell</li>
            		<li>"Vivekanand Ki Atmakatha: An Autobiography of Vivekananda" by Shri Sankar</li>
            		
            	</ul>
            </div>
            
         </div>
         <!-- Profile -->
         <div id="profile" class="tab-content">
            <div class="card">
               <h2>Profile</h2>
               <ul>
                  <li><a href="https://www.linkedin.com/in/ginni-garg/" target="_blank">Linkedin</a></li>
                  <li><a href="https://github.com/GinniIndia" target="_blank">GitHub</a></li>
                  <li><a href="https://huggingface.co/ginnigarg" target="_blank">Hugging Face</a></li>
                  <li><a href="https://scholar.google.com/citations?user=t8efybEAAAAJ&hl=en" target="_blank">Google Scholar</a></li>
                  <li><a href="https://www.webofscience.com/wos/author/record/GRO-1572-2022" target="_blank">Publons</a></li>
                  <li><a href="https://www.kaggle.com/gargginni01">Kaggle</a></li>
               </ul>
            </div>
         </div>
         <div id="contact" class="tab-content">
            <div class="card">
               <h2>Contact</h2>
               <p><b>Email:</b> <a href="mailto:gargginni01@gmail.com">gargginni01@gmail.com</a></p>
            </div>
         </div>
     
      
      <div id="evolution" class="tab-content">
         <div class="card">
            <h2>Evolution of NLP</h2>
            <p><b>1. Frequency based Vectors:</b> TF-IDF, BM25 etc, for each sentence one Frequency vector, we donot call it embedding vector as no semantic meaning.
Size of Vector is V : Vocabulary Size and Sparse, we cannot compress to small dimension D becoz it donot contain any semantic information.<br>
	    Remove Stopword/lemmitization because they are noise as embedding depend upon frequency of token<br><br>
	    <b>2. Static Embeddings (Semantic but not contextual):</b> Word2Vec, Glove etc, for each token one embedding vector. <b>(Single One Co-occurance Matrix for entire training corpus)</b>
As each token represents static embedding, donot have context from other tokens of sentence, so to get Sentence Static Embedding we apply Pooling (Mean/Max/Sum) etc over all computed token embeddings of sentence.<br>
	    <b>Drawback:</b> Its same for every token irrespective of words-context how and where it is used.<br>
	    Size of Co-occurance Matrix is V*V : Size of Vocabulary, these sparse vectors are convert for each token from V size to D, Dense Vectors which are used as inputs to MLP.<br><br>
	    <b>3. Contextual Embeddings (Semantic and Contextual): RNN/LSTM</b>, for each token one embedding vector. Not use positional encoding because its sequential so now the positions of tokens. (hidden layer provide contextual embedding)<br>
	    <b>Drawback:</b> Not scalable, sequential, slow to train<br><br>
	    <b>4. Contextual Embedding using Attention (Semantic and Contextual):</b>4.Transformers (Scalable, Parallel not Sequential). We stop generating next token when we reached <eos> or max_token_limit.<br>
	    <b>Static Embedding Matrix (E) : E(token_id):</b> Lookup is fast as indexing is fast for integers over string. Token IDs exist only for speed, batching, and GPU efficiency<br>
	    <b>KV Cache:</b> We cache K,V values for all the previous tokens, then just compute K,V values for new token at a time and reuse cached K,V values for previous token then we apply attention formula. (Speed Up)<br>
	    Dimension of Static Embedding : V * d_model<br>
	    Dimension of Positional Embedding : Context_Window * d_model<br>
	    Static Embedding + Positional Embedding + Segment Embedding == new embedding vector + Attention  == Contextual Embedding<br><br>
	    
	    <b>Encoder Only (Bidirectional)</b><br>
	    1. Sees left and right context.<br>
	    2. BERT does not do next-token prediction.<br>
	    3. Predict token : use that token’s embedding.<br>
	    4. Predict sentence : use [CLS] embedding.<br>
	    5. Middle token see more context then last token.<br><br>
	    
	    <b>Decorder Only (Unidirectional : left to right) / autoregressive</b><br>
	    1. The last token embedding has seen all previous tokens, last token itself summary token.<br>
	    2. Attention mask is left-to-right.<br>
	    3. No [cls] and [sep] tokens at start and end.<br>
	    a) "I love machine learning"<br>
	    b) ['I', 'Ġlove', 'Ġmachine', 'Ġlearning']<br>
	    c) G’ means space before the word<br>
	    d) [40, 1842, 4570, 4673]<br><br>
	    
	    <b>Contextual Embedding</b> single token is sufficent to sentence summary classification.<br>
	    <b>Static Embedding</b> Pool of all token embeddings is required to get sentence summary classification.<br><br>
	    
	    <b>5. In both Pruning and Quantization we donot change Bias. Pruning focus on weights only, while Quantization focus on weights + Activations both.</b><br><br>
	    
	    Pruning can be done both during inference or at fine-tune/training as well.</b><br>
	    <b>Unstructured Pruning:</b> Removes individual weights to zero, makes matrix sparse, matrix shape remain same, so computation remains same, donot give real speedup, reserach purpose used, 						accuracy is more, at each layer applied.<br>
	    <b>Structured Pruning:</b> Removes whole neurons / filters / channels, actual speed up, size shrink for effective pruned model, accuracy is less, at each layer applied.<br>
	    <b>Global Pruning (Prefered One):</b> Unstructured Pruning, applied across multiple layers of model.<br><br>
	    
	    <b>Quantization:</b> Quantization Scaling Factors<br>
	    <b>Static Quantization  (Post Training Method):</b> weights (pre-compute) + activations (pre-compute) use Caliber over test_loader, Need of QuantStub and deQuantStub, Ex Computer Vision<br>
	    <b>Dynamic Quantization (Post Training Method):</b> weights(pre-compute) + activations (dynamic-compute), Ex NLP, LLM<br>
	    <b>QAT:</b> Quantization Aware Training, form of Static Quantization fuse_model (Conv+BN+ReLU, Conv+BN, Conv+ReLU, Linear+ReLU), Need of QuantStub and deQuantStub<br><br>
	    
	    <b>6. Four types of model:</b> Regression, Classification, Embedding, Segmentation.<br>
	    <b>Regression:</b> Mean Squared Error Loss Function<br>
	    <b>Classification:</b> CrossEntropyLoss<br>
	    <b>Embedding (Margin Based):</b> TripletMarginLoss, Contrastive Loss, (Softmax with Additive Angular Margin + CrossEntropy : ArcFace)<br>
	    <b>Segmentation:</b> Soft Dice Loss, U-Net, only Convolution Layers<br><br>
	    
	    7. We can use <b>ROC (Receiver Operating Characteristic)</b> to have graph between True Positive Rate, False Positive Rate, then we find closet point to (0,1), so we get threshold beyond which 1 else 0 for output class.<br>
	    <b>(threshold, (False Positive Rate, True Positive Rate))</b>, this is how we can choose threshold, AUC is area under curve of ROC.<br><br>
	    
            </p>
         </div>
      </div>
      
      <div id="face-ai-demo" class="tab-content">
	  <div class="iframe-wrapper">
	    <iframe
	      src="https://ginnigarg-facial-recognisation-portfolio.hf.space"
	      allow="camera; microphone"
	      loading="lazy">
	    </iframe>
	  </div>
      </div>

      </div>
      
      <!-- Floating Chatbot Code (place here) -->
      <div class="chatbot-widget">
         <div class="chatbot-header" onclick="toggleChatbot()">
            💬 Ask Ginni
            <span id="chatbot-close">−</span>
         </div>
         <div class="chatbot-body">
            <iframe
               src="https://ginnigarg-portfolio-chatbot.hf.space"
               frameborder="0"
               width="850"
               height="450"
               ></iframe>
         </div>
      </div>
      <div class="chatbot-toggle" id="chatbot-toggle" onclick="toggleChatbot()">💬</div>
      <script>
         function openTab(tabId) {
           document.querySelectorAll('.tab-content').forEach(el => el.classList.remove('active'));
           document.querySelectorAll('.tab').forEach(el => el.classList.remove('active'));
           document.getElementById(tabId).classList.add('active');
           event.target.classList.add('active');
         }
         
         
         function toggleChatbot() {
         const widget = document.querySelector(".chatbot-widget");
         const toggleBtn = document.getElementById("chatbot-toggle");
         
         if (widget.style.display === "none" || widget.style.display === "") {
         widget.style.display = "flex";
         toggleBtn.style.display = "none";
         } else {
         widget.style.display = "none";
         toggleBtn.style.display = "flex";
         }
         }
      </script>
   </body>
</html>
